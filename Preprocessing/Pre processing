Features:
    categorical / nominal / continuous

Feature Encoding:
    categorical --> one-of-K or one-hot --> order is important --> cities
    bag of word --> uses a feature vector with an element for each of the words in the corpus's vocabulary to represent each document.
    bag-of-words with TF-IDF weights -->  create feature vectors that encode the frequencies of words
        we can see that words that are common to many of the documents in the corpus, such as sandwich, have been penalized.
    Space-efficient feature vectorizing with the hashing trick



High-dimensional feature vectors that have many zero-valued elements are called sparse vectors.
problems:
    The first problem is that high-dimensional vectors require more memory than smaller vectors.
    The second problem is known as the curse of dimensionality, or the Hughes effect.
        As the feature space's dimensionality increases,
        more training data is required to ensure that there are enough training instances with each combination of the feature's values.

solutions:
    1. stop word filtering
    2. Stemming and lemmatization
        Lemmatization is the process of determining the lemma, or the morphological root, of an inflected word based on its context. --> use lexical resource
        Stemming removes all patterns of characters that appear to be affixes, resulting in a token that is not necessarily a valid word. --> use rules

