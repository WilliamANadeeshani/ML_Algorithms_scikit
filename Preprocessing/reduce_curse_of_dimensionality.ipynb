{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n [0 1 1 1 1 0 0 0]\n [1 0 0 0 0 0 1 0]]\n{u'duke': 2, u'basketball': 1, u'lost': 4, u'played': 5, u'game': 3, u'sandwich': 6, u'unc': 7, u'ate': 0}\n{u'sandwich': 2, u'ate': 0, u'sandwiches': 3, u'eaten': 1}\ngather\ngathering\ngather\nStemmed: [['He', 'ate', 'the', u'sandwich'], [u'everi', 'sandwich', u'wa', 'eaten', 'by', 'him']]\n[[('He', 'PRP'), ('ate', 'VBD'), ('the', 'DT'), ('sandwiches', 'NNS')], [('Every', 'DT'), ('sandwich', 'NN'), ('was', 'VBD'), ('eaten', 'VBN'), ('by', 'IN'), ('him', 'PRP')]]\nLemmatized: [['He', u'eat', 'the', u'sandwich'], ['Every', 'sandwich', u'be', u'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game',\n",
    "    'I ate a sandwich'\n",
    "]\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print vectorizer.fit_transform(corpus).todense()\n",
    "print vectorizer.vocabulary_\n",
    "\n",
    "# Stemming and lemmatization\n",
    "corpus_1 = [\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "vectorizer2 = CountVectorizer(stop_words='english')\n",
    "vectorizer2.fit_transform(corpus_1).todense()\n",
    "print vectorizer2.vocabulary_\n",
    "\n",
    "corpus3 = [\n",
    "    'I am gathering ingredients for the sandwich.',\n",
    "    'There were many wizards at the gathering.'\n",
    "]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "print lemmatizer.lemmatize('gathering', 'v')\n",
    "print lemmatizer.lemmatize('gathering', 'n')\n",
    "print stemmer.stem('gathering')\n",
    "\n",
    "\n",
    "# pos tagging and stem/lemma\n",
    "wordnet_tags = ['n', 'v']\n",
    "corpus4 = [\n",
    "    'He ate the sandwiches',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "\n",
    "print 'Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus4]\n",
    "\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus4]\n",
    "print tagged_corpus\n",
    "\n",
    "print 'Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
